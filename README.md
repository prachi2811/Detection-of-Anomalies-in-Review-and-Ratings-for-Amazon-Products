# Detection-of-Anomalies-in-Review-and-Ratings-for-Amazon-Products
To Identify any inconsistencies or anomalies between the ratings and the reviews given by a customer, thereby confirming if the rating is in line with the review
Amazon.com is an American multinational technology firm headquartered in Seattle, Washington. The company was founded in 1994 and has its stock on the National Association of Securities Dealers Automated Quotations (NASDAQ) exchange. The company had started as an online marketplace for books but expanded to sell electronics, software, video games, apparel, furniture, food, toys, and jewelry. In 2015, Amazon surpassed Walmart as the most valuable retailer in the United States by market capitalization. In 2018, the two-day delivery service, Amazon Prime was announced that had surpassed 100 million subscribers worldwide.
Amazon sells more than 12 million products, not including books, media, wine, and services. When Amazon Marketplace sellers are also counted in, the total product count surges to more than 353 million.
Our data is a list of over 67000 consumer reviews for Amazon products such as Kindle, Fire TV Stick, and more. The dataset includes basic product information, rating, reviews, text, and more for each product provided by Datafiniti's Product Database. Datafiniti gives access to web data without having to set up a web scraper. The dataset includes basic product information, rating, review text, and more for each product.
This dataset has been taken from Kaggle. The link hereby is Consumer Reviews of Amazon Products. The dataset described has been obtained because of combining three individual datasets found in the aforementioned link. There are 67992 records and ten attributes. 'https://www.kaggle.com/datafiniti/consumer-reviews-of-amazon-products'
For training our model, we have taken another merged dataset that combines the following three datasets: ● imdb.com ● amazon.com ● yelp.com. These datasets were created for a paper: From Group to Individual Labels using Deep Features',Kotzias et. al, KDD 2015. It contains sentences labeled with positive or negative sentiment, extracted from reviews of movies, products, and restaurants, respectively.
Each of the records has a score that is either 1 (for the positive sentiment) or 0 (for the negative sentiment). For each website, there exist 500 positives and 500 negative sentences. These were selected randomly for larger datasets of reviews. Hence, we have 3000 records in total in this combined merged rating dataset.
The pre-processing/ cleaning text follows certain steps such as removal of punctuations like “,”, “!” and “.” , special characters like ‘â’ , stop words like a, an, the etc. Used NLTK Toolkit for data preprocessing.
Removing 2, 3, 4 ratings from the dataset as it is impossible to rectify anomalies, visualising the dataset.
Sentiment analysis method is performed using TextBlob text polarity to make training records sufficient for model evaluation. 
Metrics scores of TF-IDF are better, therefore it is chosen for Text to numeric conversion with number of words in range 1,2 i.e including both unigram and bigram words.
In order to achieve the best results different linear and non-linear models on the basis of different scoring metrics namely: Accuracy, Roc_Auc, F1-Score  were build using K-Fold Cross Validation, where Number of Splits is considered as 7.
Without any hyperparameter tuning 7 models are checked and XGB stands out with the least bias and variance trade off.
Since XGB Classifier gave the best Bias Accuracy (BA) and Variance Error (VE) Trade off. Thus choosing the best hyperparameters using Grid- Search CV. The 3 main hyperparameters considered are : n_estimators, max_depth, learning_rate.
After tuning the threshold to different values using XGB Classifier, 0.4 threshold gave the best results.
A significant improvement has been observed after threshold tuning for the XGB Classifier Model in order to detect and predict anomalies. The optimal threshold value is 0.4.
The objective is: “Detection of Anomalies in Review & Ratings for Amazon Products”. Thus it came to know that False Negatives (FN) are much more costlier than the False Positives (FP).
# Business Recommendation:
FP only costs a message that is sent to the user whereas FN affects the users’ trust on the website as it directly affects the ratings.
So to the client, recommendation can be to send a text message/email to the user to inform them about the difference between their rating and review thus allowing them to change their rating or review. Hence, the rating will be true to users’ experience. 
